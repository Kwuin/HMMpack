---
title: "Hidden Markov Model Analysis with Stan"
author: "Jingkun Lin"
date: "`r Sys.Date()`"
output:
  pdf_document:
    toc: true
    citation_package: natbib
  html_document:
    toc: true
    toc_float: true
    code_folding: show
bibliography: references.bib
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

```

## R Markdown

Hidden Markov Model [1] is widely used for modeling series and it's logically clear for Bayesian inference, for it's forward generating process is explicit. A series of latent status is derived from a hidden Markov process and the data we see comes from a distribution parameterized by the latent status and other parameters invariant to state. The psoterior distribution given a series would be 
$$
p(\mathbf{X}, \mathbf{Z} | \theta) = p(z_1 | \pi) \left[ \prod_{n=2}^{N} p(z_n | z_{n-1}, \mathbf{A}) \right] \prod_{m=1}^{N} p(x_m | z_m, \phi)
$$

Where $\mathbf{X}$ is the vector of the sequential data, $\mathbf{Z}$ is the vector of the sequential latent status. $\mathbf{A}$ and $\phi$ are parameters governing the whole model, $\pi$ is the marginal distribution for starting states. $\theta$ is the collection of model parameters. 

Given a proper prior of the model parameters, we can derive a posterior easily and we can do Bayesian inference by sampling methods. One potential choice is Hamiltonian Monte Carlo which has been well optimized in package \textbf{stan}. A \textbf{R} interface \textbf{Rstan}[2] is already developed, which makes it easier and faster than rewriting the Monte Carlo algorithms manually. 


Here's the complete markdown file in a single block that you can copy:


# Introduction

This vignette demonstrates the implementation and analysis of a Hidden Markov Model (HMM) with Gaussian emissions using Stan. We'll cover:

1. Model specification
2. Data simulation
3. Model fitting
4. Diagnostic analysis
5. Results visualization

The HMM model we're using has the following structure:

$$
z_t \sim \text{Categorical}(A_{z_{t-1},\cdot})
$$

$$
y_t \sim \mathcal{N}(\mu_{z_t}, \sigma_{z_t}^2)
$$

where:
- $z_t$ is the hidden state at time $t$
- $A$ is the transition matrix
- $y_t$ is the observation at time $t$
- $\mu_{z_t}$ and $\sigma_{z_t}$ are the mean and standard deviation for state $z_t$

## Setup

First, let's load required packages and source functions.
```{r}
library(rstan)
library(bayesplot)
source("R/sampler.R")
source("R/models.R")
```

## Initialize Parameters

Set up model parameters and true values for simulation.

```{r init}
# Set seed for reproducibility
seed <- 123
set.seed(seed)

# Model dimensions
N <- 200  # sequence length
K <- 3    # number of states

# True parameters
pi_true <- c(0.6, 0.3, 0.1)
A_true <- matrix(c(0.8, 0.1, 0.1,
                   0.1, 0.8, 0.1,
                   0.1, 0.1, 0.8),
                 nrow = K, byrow = TRUE)
mu_true <- c(-3, 0, 3)
sigma_true <- c(0.5, 0.5, 0.5)
```

## Model construction
First we need a class to represent models. Given the model parameters $\theta$, it should contain some basic functionalities and variables of a model instance. 
In our case, we use 2 classes to represent the hidden model and a model with gaussian distribution as emission distribution. They take the number of states, the true transition matrix and true starting distribution, the standard variance of emission distribution as parameters and also a random seed to ensure reproductiblitiy. 


```{r data_gen}
hmm_model <- HMM(K, A_true, pi_true, seed = seed)
hmm_gaussian_model <- HMM_Gaussian_Model(hmm_model, sigma_true^2)
```
## Synthetic data generator
Given a model instance, the synthetic data generator would first use the Markov Chain to generate a series of latent status and then sample from the specified distribution to get the data $\mathbf{X}$. The length of the sequence, the transition matrix and the parameters for the sampling distribution should be given. Also the starting state should come from the given marginal distribution $\pi$. The generator is wrapped in a method of both the latent model and the model with emission. 

```{r}
y <- hmm_gaussian_model$generate_gaussian_observations(N)$observations
y
```

```{r}
# Prepare data for Stan
stan_data <- list(
  N = N,
  K = K,
  y = y
)
```

## Model Initialization

Define initialization function for Stan.

```{r init_fun}
init_fun <- function() {
  list(
    pi = rep(1/K, K),
    A = matrix(1/K, K, K),
    mu = sort(rnorm(K, mean=mean(stan_data$y), sd=sd(stan_data$y))),
    sigma = rep(sd(stan_data$y), K)
  )
}
```

## HMC sampler 
With \textbf{Rstan} interface and code written in \text{Stan} in a separated file, we can do the sampling process automatically.Based on the samples, we can give the estimation of the model parameters and also the uncertainty quantification.


```{r fit, results='hide', message=FALSE}
fit <- stan_fit("hmm.stan", stan_data)
```

## Diagnostic Functions

We'll now examine various diagnostics to assess model convergence and fit.

### Trace Plots
Trace plots help assess mixing and convergence of the MCMC chains.

```{r trace_plots, fig.width=10, fig.height=8}
matrix_param_names <- c(outer(1:K, 1:K, FUN=function(i,j) paste0("A[", i, ",", j, "]")))
param_names <- c(matrix_param_names,
                paste0("mu[", 1:K, "]"),
                paste0("sigma[", 1:K, "]"))

rstan::traceplot(fit, param_names)
```
We can see that for each parameters the chains of samplers havae mixed. 

### Effective Sample Size Ratio
The effective sample size ratio indicates how many effective samples we get relative to the total number of samples.

```{r neff_ratio, fig.width=10, fig.height=6}
neff_ratio(fit, param_names)
```

### Autocorrelation Plots
Autocorrelation plots show the correlation between samples at different lags.

```{r acf_plots, fig.width=10, fig.height=8}
mcmc_acf(fit, param_names)
```
Here in the plots we don't see any significant correlation at lags, which means the samples are independent enough. 
### Density Plots
Posterior density plots show the distribution of parameter estimates.

```{r density_plots, fig.width=10, fig.height=8}
n_params <- length(param_names)
n_cols <- min(3, n_params)
n_rows <- ceiling(n_params/n_cols)
par(mfrow=c(n_rows, n_cols), mar=c(4,4,2,1))
plot_densities(fit, param_names)
```

### Posterior Predictive Check
Compare simulated data from the posterior with observed data to assess model fit.

```{r ppc, fig.width=8, fig.height=6}
posterior_predictive_check(fit, y, N)
```

## Results Analysis

Compare estimated parameters with true values and assess convergence diagnostics. The 

```{r results}

results <- evaluate_stan_fit(fit, param_names, y, N, K)

# Print summary statistics
print("Parameter Summaries:")
print(results$summary)

# Check convergence issues
if(length(results$rhat_problems) > 0) {
  print("Parameters with convergence issues:")
  print(param_names[results$rhat_problems])
}

if(length(results$neff_problems) > 0) {
  print("Parameters with low effective sample size:")
  print(param_names[results$neff_problems])
}

# Compare with true values
print("Comparing estimates with true values:")
# For transition matrix
for(i in 1:K) {
  for(j in 1:K) {
    true_val <- A_true[i,j]
    est_val <- results$summary[paste0("A[",i,",",j,"]"), "Mean"]
    cat(sprintf("A[%d,%d]: True = %.3f, Estimated = %.3f\n",
                i, j, true_val, est_val))
  }
}

# For means and standard deviations
for(i in 1:K) {
  cat(sprintf("mu[%d]: True = %.3f, Estimated = %.3f\n",
              i, mu_true[i], results$summary[paste0("mu[",i,"]"), "Mean"]))
  cat(sprintf("sigma[%d]: True = %.3f, Estimated = %.3f\n",
              i, sigma_true[i], results$summary[paste0("sigma[",i,"]"), "Mean"]))
}
```

# Conclusion

This vignette demonstrated the complete workflow of fitting and analyzing a Hidden Markov Model using Stan, including:

1. **Parameter Estimation**: The model successfully estimated transition probabilities, and standard deviation of emission distributions for each state. It doesn't fit the starting distribution because we are doing inference on 1 single sequence of samples which is not sufficient to get a proper estimation of the starting distribution.
2. **MCMC Diagnostics**: Through trace plots, effective sample sizes, and autocorrelation plots, we assessed the quality of our MCMC sampling.We can see that after using 4 chains with 2000 iteration and 1000 warmup, the sampler mixed well. 
3. **Model Fit**: Posterior predictive checks showed how well our model captures the observed data patterns. Given a small amount of data, the posterior distribution of correctly estimated transition matrix and emission standard deviation is correct. 
4. **Parameter Recovery**: Comparison with true parameters demonstrated the model's ability to recover the generating parameters. 

The results show that our HMM implementation successfully:
- Recovered the true parameter values within reasonable margins
- Achieved good MCMC convergence
- Produced predictions that match the observed data distribution

For future work, consider:
- Testing with different numbers of states
- Exploring different emission distributions
- Implementing model comparison methods


# References

[1] Bishop, C. M. (2006). Pattern recognition and machine learning. Springer.

[2] Stan Development Team. (2024). RStan: the R interface to Stan. R package version 2.26.22. https://mc-stan.org/




